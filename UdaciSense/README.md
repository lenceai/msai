# UdaciSense: Optimized Object Recognition

A comprehensive model compression pipeline for mobile deployment of household object recognition. This project demonstrates how to reduce model size, improve inference speed, and maintain accuracy through knowledge distillation, quantization, and graph optimization techniques.

## üéØ Project Goals

Optimize a pre-trained MobileNetV3-Small model for mobile deployment while meeting these requirements:

| Requirement | Target | Achieved (Pipeline 1) | Achieved (Pipeline 2) |
|-------------|--------|----------------------|----------------------|
| Size Reduction | ‚â•30% | ‚úÖ 38.1% | ‚úÖ 97%+ |
| Speed Improvement | ‚â•50% | ‚ö†Ô∏è 22.9% | ‚úÖ 75%+ |
| Accuracy Preservation | ‚â§5% drop | ‚úÖ 0.7% drop | ‚úÖ <5% drop |

**Two optimization pipelines are provided:**
- **Pipeline 1 (Conservative)**: Distill ‚Üí Quantize ‚Üí TorchScript ‚Äî Best accuracy preservation
- **Pipeline 2 (Aggressive)**: Aggressive Distillation (Tiny Model) ‚Üí TorchScript ‚Äî Meets 50% speed target

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- PyTorch 2.0+ (2.7.1 recommended)
- CUDA 12.x (optional, for GPU acceleration)

### Installation

1. **Navigate to the project directory:**
```bash
cd /path/to/UdaciSense
```

2. **Install dependencies:**
```bash
# Install PyTorch (GPU version - CUDA 12.8)
pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu128

# Or CPU-only version
pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cpu

# Install remaining dependencies
pip install -r requirements.txt
```

3. **Install the project as a local package:**
```bash
pip install -e .
```

### Running the Notebooks

Execute the notebooks in order:

```bash
# Start Jupyter
jupyter lab notebooks/
```

**Workflow:**
1. `01_baseline.ipynb` - Train/load baseline model and establish performance metrics
2. `02_compression.ipynb` - Experiment with individual compression techniques
3. `03_pipeline.ipynb` - Run the multi-stage optimization pipelines
4. `04_deployment.ipynb` - Convert to mobile-ready format

## üìÅ Project Structure

```
UdaciSense/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 00_setup_check.py        # Verify environment setup
‚îÇ   ‚îú‚îÄ‚îÄ 01_baseline.ipynb        # Baseline model training & evaluation
‚îÇ   ‚îú‚îÄ‚îÄ 02_compression.ipynb     # Individual compression experiments
‚îÇ   ‚îú‚îÄ‚îÄ 03_pipeline.ipynb        # Multi-stage optimization pipelines
‚îÇ   ‚îî‚îÄ‚îÄ 04_deployment.ipynb      # Mobile deployment conversion
‚îÇ
‚îú‚îÄ‚îÄ compression/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ in_training/             # Training-time compression
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ distillation.py      # Knowledge distillation + Tiny model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pruning.py           # Gradual magnitude pruning
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ quantization.py      # Quantization-aware training (QAT)
‚îÇ   ‚îî‚îÄ‚îÄ post_training/           # Post-training compression
‚îÇ       ‚îú‚îÄ‚îÄ graph_optimization.py # TorchScript & Torch FX
‚îÇ       ‚îú‚îÄ‚îÄ pruning.py           # L1/structured pruning
‚îÇ       ‚îî‚îÄ‚îÄ quantization.py      # Dynamic/static quantization
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Constants (TARGET_INFERENCE_SPEEDUP=0.50)
‚îÇ   ‚îú‚îÄ‚îÄ compression.py           # Experiment tracking utilities
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py           # CIFAR-100 household subset loader
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py            # Metrics: accuracy, timing, size
‚îÇ   ‚îú‚îÄ‚îÄ mobile_deployment.py     # Mobile conversion utilities
‚îÇ   ‚îú‚îÄ‚îÄ model.py                 # MobileNetV3_Household model
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py         # Plotting functions
‚îÇ
‚îú‚îÄ‚îÄ models/                      # Saved model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ baseline_mobilenet/      # Baseline model
‚îÇ   ‚îî‚îÄ‚îÄ pipeline/                # Optimized pipeline models
‚îÇ
‚îú‚îÄ‚îÄ results/                     # Evaluation metrics (JSON)
‚îÇ   ‚îú‚îÄ‚îÄ baseline_mobilenet/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline/
‚îÇ
‚îú‚îÄ‚îÄ data/                        # Dataset (auto-downloads CIFAR-100)
‚îÇ
‚îú‚îÄ‚îÄ report.md                    # Technical report with results
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ setup.py                     # Package installation
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üîß Compression Techniques

### Implemented Techniques

| Technique | Category | Size ‚Üì | Speed ‚Üë | Accuracy Impact |
|-----------|----------|--------|---------|-----------------|
| Knowledge Distillation | In-Training | ~29% | Slight | +1-2% (improvement!) |
| Dynamic Quantization | Post-Training | ~29% | Minimal | <1% drop |
| Static Quantization | Post-Training | ~65% | ~40% | <2% drop |
| TorchScript Optimization | Post-Training | ~2% | ~20-60% (GPU) | 0% |
| L1 Unstructured Pruning | Post-Training | 0% | 0% | Needs fine-tuning |
| Structured Pruning | Post-Training | Varies | ~10-20% | 1-3% drop |

### Key Models

**MobileNetV3_Household** (Baseline):
- Parameters: 1,528,106
- Size: 5.96 MB
- CPU Inference: 5.55 ms
- Accuracy: 88.7%

**MobileNetV3_Household_Small** (Student for Pipeline 1):
- Parameters: ~1,077,000
- width_mult: 0.6

**MobileNetV3_Household_Tiny** (Student for Pipeline 2):
- Parameters: ~41,000 (97% smaller!)
- Custom depthwise separable architecture
- Input resolution: 128√ó128 (vs 224√ó224)
- 4-5√ó faster inference

## üìä Results

### Pipeline 1: Conservative (Distill ‚Üí Quantize ‚Üí TorchScript)

```
Baseline ‚Üí Distillation ‚Üí Quantization ‚Üí TorchScript ‚Üí Final
5.96 MB     4.24 MB        3.81 MB         3.69 MB
88.7%       87.9%          88.0%           88.0%
5.55 ms     5.28 ms        5.29 ms         4.28 ms
```

**Final Results:**
- Size: 3.69 MB (38.1% reduction ‚úÖ)
- Speed: 4.28 ms (22.9% improvement ‚ö†Ô∏è)
- Accuracy: 88.0% (0.7% drop ‚úÖ)

### Pipeline 2: Aggressive Speed Optimization

```
Baseline ‚Üí Aggressive Distillation (Tiny) ‚Üí TorchScript ‚Üí Final
5.96 MB     ~0.16 MB                          ~0.16 MB
88.7%       ~84-88%                           ~84-88%
5.55 ms     ~1.4 ms                           ~1.4 ms
```

**Final Results:**
- Size: ~0.16 MB (97%+ reduction ‚úÖ)
- Speed: ~1.4 ms (75%+ improvement ‚úÖ)
- Accuracy: ~84-88% (<5% drop ‚úÖ)

## üèÉ Running the Pipelines

### Pipeline 1 (Conservative)

In `03_pipeline.ipynb`, the first pipeline is already configured:

```python
pipeline1 = OptimizationPipeline(name="distill_quantize_torchscript", ...)

pipeline1.add_step("Knowledge Distillation", apply_knowledge_distillation,
                   temperature=3.0, alpha=0.7, num_epochs=15)
pipeline1.add_step("Dynamic Quantization", apply_dynamic_quantization)
pipeline1.add_step("TorchScript Optimization", apply_graph_optimization)

optimized_model = pipeline1.run(device=device)
```

### Pipeline 2 (Aggressive - 50% Speed Target)

Also in `03_pipeline.ipynb`:

```python
pipeline2 = OptimizationPipeline(name="aggressive_speed_optimization", ...)

pipeline2.add_step("Aggressive Distillation (Tiny Model)", 
                   apply_aggressive_knowledge_distillation,
                   temperature=4.0, alpha=0.7, num_epochs=30, width_mult=0.5)
pipeline2.add_step("TorchScript Optimization", apply_graph_optimization)

optimized_model = pipeline2.run(device=device)
```

## üì± Mobile Deployment

After running the pipelines, convert to mobile format in `04_deployment.ipynb`:

```python
from utils.mobile_deployment import convert_model_for_mobile

mobile_model = convert_model_for_mobile(optimized_model, input_size=input_size)

# Save for PyTorch Mobile
torch.jit.save(mobile_model, "models/mobile/optimized_model.pt")

# Save for Lite Interpreter (smaller)
mobile_model._save_for_lite_interpreter("models/mobile/optimized_model.ptl")
```

## üìã Dataset

**Household Objects** - A 10-class subset of CIFAR-100:
- Classes: clock, keyboard, lamp, telephone, television, bed, chair, couch, table, wardrobe
- Training: 5,000 images (500 per class)
- Test: 1,000 images (100 per class)
- Size: 32√ó32 RGB (interpolated to 224√ó224 or 128√ó128 for models)

The dataset downloads automatically on first run.

## üõ†Ô∏è Troubleshooting

### Common Issues

**1. CUDA Out of Memory**
```python
# Use CPU for evaluation
device = torch.device('cpu')
```

**2. Quantized Model Errors**
```python
# Quantized models must run on CPU
model = model.cpu()
model.eval()
```

**3. TorchScript Save/Load Issues**
```python
# Skip optimize_for_inference for quantized models
# This is handled automatically in graph_optimization.py
```

**4. Import Errors**
```bash
# Reinstall the package
pip install -e .
```

## üìö Documentation

- **[report.md](report.md)** - Complete technical report with analysis
- **[PROJECT_GUIDE.md](PROJECT_GUIDE.md)** - Detailed step-by-step guide
- **[IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)** - Technical implementation details

## üîó References

1. [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)
2. [MobileNetV3 Paper](https://arxiv.org/abs/1905.02244)
3. [Knowledge Distillation](https://arxiv.org/abs/1503.02531)
4. [PyTorch Mobile](https://pytorch.org/mobile/home/)
5. [TorchScript](https://pytorch.org/docs/stable/jit.html)

## üìÑ License

This project is part of the Udacity Machine Learning Nanodegree curriculum.

---

*Last updated: December 2024*
